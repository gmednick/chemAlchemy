[{"authors":["gabe"],"categories":null,"content":"My journey into the physical and biological sciences started with a desire to study osteopathic medicine. In the process of completing a biochemistry and molecular biology degree, my interest in the structure and function of the human body grew into a fascination with the invisible structure and inner workings of the cell. I developed a deep interest in both physical chemistry and biochemistry, and my curiosity resulted in a PhD focussed on sensory transduction pathways and light sensing mechanisms in bacteria.\nAfter finishing my PhD, I developed and implemented innovative teaching practices in chemistry and biology at the university level. More recently (2018-2020), I worked as a senior scientist for a small startup with an emphasis on DNA and RNA synthesis. I love hands-on research but my current professional passion is centered on data science and bioinformatics. My mission is to facilitate data informed choices that provide insight, drive innovation and optimize decision making.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d5326a2f1cf09d7e5a9f14131238744","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"My journey into the physical and biological sciences started with a desire to study osteopathic medicine. In the process of completing a biochemistry and molecular biology degree, my interest in the structure and function of the human body grew into a fascination with the invisible structure and inner workings of the cell.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":" The iris dataset The iris dataset is a classic, so much so that it comes packaged with base R. You can use the data() function to see a list of datasets that come with R. There are often datasets associated with packages as well. Try data(package = 'dplyr').\nLet’s take a look at the data.\n# load the iris data set and clean the column names with janitor::clean_names() iris_df\u0026lt;- iris %\u0026gt;% clean_names() iris_df %\u0026gt;% head() ## sepal_length sepal_width petal_length petal_width species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa iris_df %\u0026gt;% count(species) ## species n ## 1 setosa 50 ## 2 versicolor 50 ## 3 virginica 50 # equal number of each species, 150 total iris_df %\u0026gt;% str() ## \u0026#39;data.frame\u0026#39;: 150 obs. of 5 variables: ## $ sepal_length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ sepal_width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ petal_length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ petal_width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ species : Factor w/ 3 levels \u0026quot;setosa\u0026quot;,\u0026quot;versicolor\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... This dataset is clean, but with only 150 observations, we must question whether it’s too small to train a reliable model. We will generate bootstrap resamples to make the most of what we have.\nWe see that we have four features (sepal length and with, and petal length and width) and there are three unique species..\nIt makes sense to create a model that predicts the species of iris based on the flower’s measurements.\n Visualize relationships Before we do any kind of machine learning, let’s visualize the relationships in our data and get a feel for the predictive power of our features. We can do this by plotting the explanatory features and then use color or faceting to create the distinction of species.\nsepal \u0026lt;- iris_df %\u0026gt;% ggplot(aes(sepal_length, sepal_width, color = species)) + geom_point(size = 1) + facet_wrap(~species) + labs(x = \u0026#39;sepal length\u0026#39;, y = \u0026#39;sepal width\u0026#39;) petal \u0026lt;- iris_df %\u0026gt;% ggplot(aes(petal_length, petal_width, color = species)) + geom_point(size =1) + facet_wrap(~species) + labs(x = \u0026#39;petal length\u0026#39;, y = \u0026#39;petal width\u0026#39;) (petal/sepal) # patchwork allows us to arrange plots side-by-side or stacked but there\u0026#39;s a better way..  Tidy the dataset Let’s change the shape of our data by combining the four iris features into a single column (we’ll name, e.g., metric) and the associated values populate a new column (named value). This transformation into a longer dataset can be done with the function pivot_longer(). As we shall see, the tidy format lends itself to data visualization and analysis with the tidyverse.\niris_df_long \u0026lt;- iris_df %\u0026gt;% pivot_longer(cols = sepal_length:petal_width, names_to = \u0026#39;metric\u0026#39;, values_to =\u0026#39;value\u0026#39;) # A boxplot is a great way to see the median values of our features by species. iris_df_long %\u0026gt;% ggplot(aes(metric, value, color = species)) + geom_boxplot()  # A nice alternative is to facet by the metric to convey the same information but with added clarity. iris_df_long %\u0026gt;% ggplot(aes(species, value, color = species)) + geom_boxplot(alpha = 0.3) + facet_wrap(~ metric, scales = \u0026quot;free_y\u0026quot;) # The same information can be displayed by comparing the distributions in histogram. iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_histogram(bins = 20, alpha = 0.7) + facet_wrap(~ metric, scales = \u0026quot;free_x\u0026quot;) # geom_denisty is a nice alternative to geom_histogram. iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_density(alpha = .5) + facet_wrap(~ metric, scales = \u0026quot;free\u0026quot;)  Got models Before we get to modeling, we will want to split the data and typically some feature engineering may be necessary as well. Since our dataset is small, we are going to take the training set and make 25 bootstrap resamples. By default, initial split provides a 75:25 split for our train and test sets respectively. The function bootstraps will take the training data, further split it into a training and test set, then resample and repeat 25 times. This ‘resampling’ provides a more robust method to test our model validity.\nset.seed(123) tidy_split \u0026lt;- initial_split(iris_df) tidy_split ## \u0026lt;Analysis/Assess/Total\u0026gt; ## \u0026lt;113/37/150\u0026gt; iris_train \u0026lt;- training(tidy_split) iris_test \u0026lt;- testing(tidy_split) iris_boots \u0026lt;- bootstraps(iris_train) iris_boots ## # Bootstrap sampling ## # A tibble: 25 x 2 ## splits id ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;split [113/39]\u0026gt; Bootstrap01 ## 2 \u0026lt;split [113/40]\u0026gt; Bootstrap02 ## 3 \u0026lt;split [113/44]\u0026gt; Bootstrap03 ## 4 \u0026lt;split [113/41]\u0026gt; Bootstrap04 ## 5 \u0026lt;split [113/39]\u0026gt; Bootstrap05 ## 6 \u0026lt;split [113/40]\u0026gt; Bootstrap06 ## 7 \u0026lt;split [113/42]\u0026gt; Bootstrap07 ## 8 \u0026lt;split [113/35]\u0026gt; Bootstrap08 ## 9 \u0026lt;split [113/42]\u0026gt; Bootstrap09 ## 10 \u0026lt;split [113/41]\u0026gt; Bootstrap10 ## # … with 15 more rows  Recipes Recipes is a powerful tool for a wide range of feature engineering tasks that can be prepare your data for modeling. A great way to see the available functions in a package is to type the package name and two colons, eg recipes:: into the Rstudio console. All package functions will pop up and a brief description for the highlighted function will be available.\nLet’s create a recipe to demonstrate how easy it is to apply feature engineering. Since the features of the iris dataset do not actually require any feature engineering, we won’t include this recipe in our final workflow.\niris_rec \u0026lt;- recipe(species ~., data = iris_train) %\u0026gt;% step_pca(all_predictors()) %\u0026gt;% step_normalize(all_predictors()) prep \u0026lt;- prep(iris_rec) kable(head(iris_juice \u0026lt;- juice(prep)))   species PC1 PC2 PC3 PC4    setosa 1.1621621 1.384382 -0.0270717 0.0422303  setosa 1.3983644 1.159657 0.7767696 0.6840123  setosa 1.4836145 1.244589 0.0010748 0.1379599  setosa 1.4923559 1.089539 0.0331285 -0.4892659  setosa 1.1873971 1.402225 -0.4220083 -0.3874109  setosa 0.7736181 1.394953 -0.8518454 -0.1532614     Creating models with the Recipes package Let’s set up two different models: first up, the generalized linear model or glmnet. I will run each part of the code to preview the internals of the model as we go .\n# set seed set.seed(1234) # generate the glmnet model with parsnip glmnet_mod \u0026lt;- multinom_reg(penalty = 0) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) glmnet_mod ## Multinomial Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0 ## ## Computational engine: glmnet # create a workflow glmnet_wf \u0026lt;- workflow() %\u0026gt;% add_formula(species ~ .) glmnet_wf ## ══ Workflow ═══════════════════════════════════ ## Preprocessor: Formula ## Model: None ## ## ── Preprocessor ─────────────────────────────── ## species ~ . # add the model to the workflow and use iris_boots to fit our model 25 times glmnet_results \u0026lt;- glmnet_wf %\u0026gt;% add_model(glmnet_mod) %\u0026gt;% fit_resamples( resamples = iris_boots, control = control_resamples(extract = extract_model, save_pred = TRUE) ) glmnet_results ## # Resampling results ## # Bootstrap sampling ## # A tibble: 25 x 6 ## splits id .metrics .notes .extracts .predictions ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [39 ×… ## 2 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [40 ×… ## 3 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [44 ×… ## 4 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [41 ×… ## 5 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [39 ×… ## 6 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [40 ×… ## 7 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [42 ×… ## 8 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [35 ×… ## 9 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [42 ×… ## 10 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [41 ×… ## # … with 15 more rows # look at the model metrics collect_metrics(glmnet_results) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.944 25 0.00642 ## 2 roc_auc hand_till 0.987 25 0.00234 Let’s repeat the same process but switch out the model. I will choose a random forest model.\nset.seed(1234) rf_mod \u0026lt;- rand_forest() %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) # We set up a workflow and add the parts of our model together like legos rf_wf \u0026lt;- workflow() %\u0026gt;% add_formula(species ~ .) # Here we fit our 25 resampled datasets rf_results \u0026lt;- rf_wf %\u0026gt;% add_model(rf_mod) %\u0026gt;% fit_resamples( resamples = iris_boots, control = control_resamples(save_pred = TRUE) ) collect_metrics(rf_results) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.945 25 0.00764 ## 2 roc_auc hand_till 0.993 25 0.00126 Here is a look at the confusion matrix summaries for our model. The confusion matrix let’s us see the correct and incorrect predictions of our models in a single table.\nglmnet_results %\u0026gt;% conf_mat_resampled()  ## # A tibble: 9 x 3 ## Prediction Truth Freq ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa setosa 13.3 ## 2 setosa versicolor 0 ## 3 setosa virginica 0 ## 4 versicolor setosa 0 ## 5 versicolor versicolor 12.2 ## 6 versicolor virginica 1 ## 7 virginica setosa 0 ## 8 virginica versicolor 1.24 ## 9 virginica virginica 12.4 rf_results %\u0026gt;% conf_mat_resampled()  ## # A tibble: 9 x 3 ## Prediction Truth Freq ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa setosa 13.3 ## 2 setosa versicolor 0 ## 3 setosa virginica 0 ## 4 versicolor setosa 0 ## 5 versicolor versicolor 12.3 ## 6 versicolor virginica 1.04 ## 7 virginica setosa 0 ## 8 virginica versicolor 1.16 ## 9 virginica virginica 12.4 The ROC curve helps us visually interpret our model performance at every threshold.\nglmnet_results %\u0026gt;% collect_predictions() %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(species, .pred_setosa:.pred_virginica) %\u0026gt;% autoplot() rf_results %\u0026gt;% collect_predictions() %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(species, .pred_setosa:.pred_virginica) %\u0026gt;% autoplot() + theme(legend.position = \u0026#39;none\u0026#39;) # Final fit This is it! By using the last_fit(tidy_split), we are able to train our model on the training set and test the model on the testing set in one fell swoop! Note, this is the only time we use the test set.\nfinal_glmnet \u0026lt;- glmnet_wf %\u0026gt;% add_model(glmnet_mod) %\u0026gt;% last_fit(tidy_split) final_glmnet ## # Resampling results ## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113… train/test … \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [37 × … \u0026lt;workflo… final_rf \u0026lt;- rf_wf %\u0026gt;% add_model(rf_mod) %\u0026gt;% last_fit(tidy_split) final_rf ## # Resampling results ## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113… train/test … \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [37 × … \u0026lt;workflo…  Confusion Matrices Finally, let’s generate a multiclass confusion matrix with the results from the test data. The confusion matrix provides a count of each outcome for all possible outcomes. The columns contain the true values and predictions are set across rows. Notice that the correct predictions are on the diagonal and there were no incorrect predictions.\ncollect_metrics(final_glmnet) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 1 ## 2 roc_auc hand_till 1 collect_predictions(final_glmnet) %\u0026gt;% conf_mat(species, .pred_class) %\u0026gt;% autoplot(type = \u0026#39;heatmap\u0026#39;) collect_metrics(final_rf) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.973 ## 2 roc_auc hand_till 1 collect_predictions(final_rf) %\u0026gt;% conf_mat(species, .pred_class) %\u0026gt;% autoplot(type = \u0026#39;heatmap\u0026#39;)  Final thoughts It is always good to ask the question, ‘Do my results make sense’? Both models show near perfect predictive power but are they really that good? From our visual analysis, we can confidently say yes, there is a clear distinction between species when comparing the explanatory features. Now that we have a model, we have the power to predict iris type on new data.\nI would like to thank my mentors, the three noble Jedi Knights of the tidyverse and tidymodels: Julia Silge, David Robinson and Andrew Couch.\n ","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603318262,"objectID":"517bb6adcf89c779300b0648b798dbfb","permalink":"/post/iris-classification/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/post/iris-classification/","section":"post","summary":"Flower classification using Tidymodels","tags":["machine learning","tidymodels"],"title":"Iris Classification","type":"post"},{"authors":[],"categories":[],"content":" Intro to the Tidyverse There are five related datasets but we will only look at key_crop_yields dataset in this exploration. The data contains crop yield (tonnes per hectare) by country and year. Let’s get started by loading the packages that we will need for this analysis.\nThis data comes from the wonderful TidyTuesday project community learning project (2020-09-01).Let’s download the data from the R4DS Github account and take a look at it. There is the wonderful package, tidytuesdayR (Ellis Hughes), that allows one to load and explore the TidyTuesday data in the Rstudio viewer. We will use the readr package from the tidyverse using read_csv to import our data as a modern data frame or tibble.\n# Load the data with a readr function crop_yields \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv\u0026quot;) # land_use \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/land_use_vs_yield_change_in_cereal_production.csv\u0026quot;) %\u0026gt;% janitor::clean_names() # The skim function provides a great way to get an initial summary. I typically use View() as well to view the dataset in Rstudio skimr::skim(crop_yields)  Table 1: Data summary  Name crop_yields  Number of rows 13075  Number of columns 14  _______________________   Column type frequency:   character 2  numeric 12  ________________________   Group variables None    Variable type: character\n  skim_variable n_missing complete_rate min max empty n_unique whitespace    Entity 0 1.00 4 39 0 249 0  Code 1919 0.85 3 8 0 214 0    Variable type: numeric\n    skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist    Year 0 1.00 1990.37 16.73 1961.00 1976.00 1991.00 2005.00 2018.00 ▇▆▇▇▇  Wheat (tonnes per hectare) 4974 0.62 2.43 1.69 0.00 1.23 1.99 3.12 10.67 ▇▅▂▁▁  Rice (tonnes per hectare) 4604 0.65 3.16 1.85 0.20 1.77 2.74 4.16 10.68 ▇▇▃▁▁  Maize (tonnes per hectare) 2301 0.82 3.02 3.13 0.03 1.14 1.83 3.92 36.76 ▇▁▁▁▁  Soybeans (tonnes per hectare) 7114 0.46 1.45 0.75 0.00 0.86 1.33 1.90 5.95 ▇▇▂▁▁  Potatoes (tonnes per hectare) 3059 0.77 15.40 9.29 0.84 8.64 13.41 20.05 75.30 ▇▅▁▁▁  Beans (tonnes per hectare) 5066 0.61 1.09 0.82 0.03 0.59 0.83 1.35 9.18 ▇▁▁▁▁  Peas (tonnes per hectare) 6840 0.48 1.48 1.01 0.04 0.72 1.15 1.99 7.16 ▇▃▁▁▁  Cassava (tonnes per hectare) 5887 0.55 9.34 5.11 1.00 5.55 8.67 11.99 38.58 ▇▇▁▁▁  Barley (tonnes per hectare) 6342 0.51 2.23 1.50 0.09 1.05 1.88 3.02 9.15 ▇▆▂▁▁  Cocoa beans (tonnes per hectare) 8466 0.35 0.39 0.28 0.00 0.24 0.36 0.49 3.43 ▇▁▁▁▁  Bananas (tonnes per hectare) 4166 0.68 15.20 12.08 0.66 5.94 11.78 20.79 77.59 ▇▃▁▁▁    # Here are a few other options for getting to know our data # glimpse(crop_yields) # summary(crop_yields) # View(crop_yields)  Data cleaning and reshaping Let’s start with a little data cleaning! Using skim from skimr provides a convenient summary. We can see that there are plenty of NAs that we will most likely need to remove. We can also standardize our feature names with the janitor package. Lastly, we should consider the shape of our data and make sure that it’s in a tidy format where each column is a feature and each row a sample.This long or tidy format is a prerequisite for fluent analysis with the tidyverse and tidymodels frameworks. In our data example, the various crops on samples, that can be categorized as the feature crops. The function pivot_longer is fantastic function from the tidyr package that allows us to reshape our data and preparing it for This will benefit our downstream analysis, including EDA (exploratory data analysis) and modeling (machine learning).\ncrop_yields %\u0026gt;% count(Year) ## # A tibble: 58 x 2 ## Year n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1961 209 ## 2 1962 209 ## 3 1963 209 ## 4 1964 209 ## 5 1965 209 ## 6 1966 211 ## 7 1967 211 ## 8 1968 213 ## 9 1969 213 ## 10 1970 213 ## # … with 48 more rows # Use janitor package to clean column names and pivot_longer from tidyr to tidy the data. crop_yield_tidy \u0026lt;- crop_yields %\u0026gt;% janitor::clean_names() %\u0026gt;% rename_all(str_remove, \u0026quot;_tonnes.*\u0026quot;) %\u0026gt;% rename(country = entity) %\u0026gt;% pivot_longer(wheat:bananas, names_to = \u0026#39;crop\u0026#39;, values_to = \u0026#39;yield_hectare\u0026#39;) %\u0026gt;% drop_na(yield_hectare) kable(head(crop_yield_tidy, n = 10))   country code year crop yield_hectare    Afghanistan AFG 1961 wheat 1.0220  Afghanistan AFG 1961 rice 1.5190  Afghanistan AFG 1961 maize 1.4000  Afghanistan AFG 1961 potatoes 8.6667  Afghanistan AFG 1961 barley 1.0800  Afghanistan AFG 1962 wheat 0.9735  Afghanistan AFG 1962 rice 1.5190  Afghanistan AFG 1962 maize 1.4000  Afghanistan AFG 1962 potatoes 7.6667  Afghanistan AFG 1962 barley 1.0800    # crop_yield_tidy %\u0026gt;% # summary() # # crop_yield_tidy %\u0026gt;% # count(year, country, sort = TRUE) # # tidy_select \u0026lt;- crop_yield_tidy %\u0026gt;% # count(country, sort = TRUE) %\u0026gt;% # filter(n \u0026gt;=500) # # crop_yield_tidy %\u0026gt;% # summarize(max(year)- min(year)) #57 years of recorded data # # crop_yield_tidy %\u0026gt;% # count(year, crop, sort = T) # # crop_yield_tidy %\u0026gt;% # filter(country == \u0026#39;United States\u0026#39;) %\u0026gt;% # count(crop)  Visual analysis Now that we have the data in tidy format, let’s see what we can learn from it. We want to look at crop production by country but there are too many countries to look at in a single plot. slice_sample() can be used to randomly select a subset of the total countries. We will use the results to visualize a subset of the countries with the idea that this will help us get a feel for relationships in the data. For more refined control of the country selection, we could incorporate the code into a Shiny dashboard with user conntrolled selection inputs.\n# Randomly select 9 country names # set.seed(1014) if you want the same 9 countries to be selected each time country_random \u0026lt;- crop_yield_tidy %\u0026gt;% add_count(year, crop) %\u0026gt;% filter(crop \u0026gt;= 58) %\u0026gt;% select(country) %\u0026gt;% distinct() %\u0026gt;% slice_sample(n = 9) %\u0026gt;% pull() # another option would be to set the code equal to a set selection of countries: E.g., countries \u0026lt;- c(\u0026#39;GUM\u0026#39;, \u0026#39;IND\u0026#39;, \u0026#39;YEM\u0026#39;, \u0026#39;NAM\u0026#39;, \u0026#39;URY\u0026#39;, \u0026#39;USA\u0026#39;) # Plot crop yield per hectare. library(tidytext) usa_col_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(country == \u0026#39;United States\u0026#39;) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(yield_hectare, crop, fill = crop)) + geom_col() + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) + labs( title = \u0026#39;Crop Yield USA\u0026#39;, y = \u0026#39;Crop\u0026#39;, x = \u0026#39;tonnes per hectare\u0026#39; ) usa_col_plot column_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(country == country_random) %\u0026gt;% mutate(crop = reorder_within(crop, yield_hectare, country)) %\u0026gt;% ggplot(aes(yield_hectare, crop, fill = crop)) + geom_col() + scale_y_reordered() + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) + labs( title = \u0026#39;Crop Yield\u0026#39;, y = \u0026#39;Crop\u0026#39;, x = \u0026#39;yield per hectare\u0026#39; ) column_plot Figure 1. This column plot shows the crop on the y-axis, yield on the x-axis and is faceted by country.\n# Plot crop yield per hectare per year. line_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(year \u0026gt;= 2000, country == country_random) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(year, yield_hectare, color = crop)) + geom_line() + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + labs( y = \u0026#39;yield per hectare\u0026#39;, title = \u0026#39;Crop Yield\u0026#39; ) line_plot # Plot crop yield per hectare per year. box_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(year \u0026gt;= 2000, country == country_random) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(yield_hectare, crop, color = crop)) + geom_boxplot() + theme(legend.position = \u0026#39;none\u0026#39;) + facet_wrap(~country, scales = \u0026#39;free\u0026#39;) + labs( x = \u0026#39;Yield\u0026#39;, title = \u0026#39;Crop Yield\u0026#39;, y = \u0026#39;\u0026#39; ) box_plot  ","date":1599091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"7ab900960e472a8bde7aa822b075d5b9","permalink":"/post/hello-tidyverse/","publishdate":"2020-09-03T00:00:00Z","relpermalink":"/post/hello-tidyverse/","section":"post","summary":"Data wrangling and visualization: insights into data analysis with the tidyverse","tags":[],"title":"Hello Tidyverse and Tidymodels","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]