[{"authors":["gabe"],"categories":null,"content":"During graduate school I maintained a broad interdisciplinary approach that included microbiology, molecular biology, biochemistry, physical chemistry and spectroscopy. As a postdoc, I developed curriculum and taught biology and chemistry with an emphasis on interactive, inquiry-based learning. In my first role as a lead scientist with a start-up, I worked on proprietary DNA and RNA synthesis. Over the last few years, my insatiable interest in bioinformatics and data science has transformed my skill-set and expanded my professional repertoire.\nI enjoy the challenge of drawing insight from all types of data. Each step of the process, from data cleaning and exploration to creating models, deriving predictions and communicating results, is a blend of art and science. Trying to find that perfect blend is what drives my passion as a data scientist.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d5326a2f1cf09d7e5a9f14131238744","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"During graduate school I maintained a broad interdisciplinary approach that included microbiology, molecular biology, biochemistry, physical chemistry and spectroscopy. As a postdoc, I developed curriculum and taught biology and chemistry with an emphasis on interactive, inquiry-based learning.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":" This is a work in progress (like all of my posts). This app currently allows one to visualize the number of new cases and new deaths over time. The default States are California, Hawaii and Florida but you can add or remove States using the selector box in the sidebar. The accompanying maps use color to show the total cases and total deaths per million people by State. The data is from Johns Hopkins University and is updated daily.   ","date":1604620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604698729,"objectID":"869f54371b2c9d8c2cfc4e3c14e93b6c","permalink":"/post/covid-shiny-app/","publishdate":"2020-11-06T00:00:00Z","relpermalink":"/post/covid-shiny-app/","section":"post","summary":"Covid cases and deaths in the United States","tags":[],"title":"COVID Shiny App","type":"post"},{"authors":[],"categories":[],"content":" Iris data The iris dataset is a classic, so much so that it’s included in the datasets package that comes with every installation of R. You can use data() to see a list of all available datasets. Datasets that are associated with packages can be found in a similar way, e.g., data(package = 'dplyr').\nLet’s take a look at the data.\n# load the iris data set and clean the column names with janitor::clean_names() iris_df\u0026lt;- iris %\u0026gt;% clean_names() iris_df %\u0026gt;% head() ## sepal_length sepal_width petal_length petal_width species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa iris_df %\u0026gt;% count(species) ## species n ## 1 setosa 50 ## 2 versicolor 50 ## 3 virginica 50 # equal number of each species, 150 total iris_df %\u0026gt;% str() ## \u0026#39;data.frame\u0026#39;: 150 obs. of 5 variables: ## $ sepal_length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ sepal_width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ petal_length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ petal_width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ species : Factor w/ 3 levels \u0026quot;setosa\u0026quot;,\u0026quot;versicolor\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The data is clean but with only 150 observations, it’s worth questioning whether a reliable model can be generated. We’ll get back to this question. There are four variables or features (sepal length and width, and petal length and width) in our data and three unique species.\nLet’s use the data to create a classification model that predicts the species of iris based on a flower’s measurements.\n Visualize relationships Before we do any kind of machine learning, it’s helpful to visualize the data and develop a better understanding of the variables as well as their relationships. This will also give us a stronger intuitive sense about the potential predictive power of the data.\nsepal \u0026lt;- iris_df %\u0026gt;% ggplot(aes(sepal_length, sepal_width, color = species)) + geom_point(size = 1) + facet_wrap(~species) + labs(x = \u0026#39;sepal length\u0026#39;, y = \u0026#39;sepal width\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) petal \u0026lt;- iris_df %\u0026gt;% ggplot(aes(petal_length, petal_width, color = species)) + geom_point(size =1) + facet_wrap(~species) + labs(x = \u0026#39;petal length\u0026#39;, y = \u0026#39;petal width\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) (petal/sepal) # patchwork allows us to arrange plots side-by-side or stacked but there\u0026#39;s a better way..  Tidy the dataset Let’s change the shape of our data by combining the four iris features into a single column (named metric) and the associated values will populate a new column (named value). This transformation into a longer dataset can be achieved with the function pivot_longer(). As we shall see, the tidy format lends itself to data visualization and analysis with the tidyverse.\niris_df_long \u0026lt;- iris_df %\u0026gt;% pivot_longer(cols = sepal_length:petal_width, names_to = \u0026#39;metric\u0026#39;, values_to =\u0026#39;value\u0026#39;) # A boxplot is a great way to see the median values of our features by species. iris_df_long %\u0026gt;% ggplot(aes(metric, value, color = species)) + geom_boxplot()  # A nice alternative is to facet by the metric to convey the same information but with added clarity. iris_df_long %\u0026gt;% ggplot(aes(species, value, color = species)) + geom_boxplot(alpha = 0.3) + facet_wrap(~ metric, scales = \u0026quot;free_y\u0026quot;) # The same information can be displayed by comparing the distributions in histogram. iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_histogram(bins = 20, alpha = 0.7) + facet_wrap(~ metric, scales = \u0026quot;free_x\u0026quot;) # geom_denisty is a nice alternative to geom_histogram. iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_density(alpha = .5) + facet_wrap(~ metric, scales = \u0026quot;free\u0026quot;)  Get modelling Before we get to modeling, we need to split the data. Since our dataset is small, we are going to take the training set and make bootstrap resamples. By default, initial split provides a 75:25 split for our train and test sets respectively. The function bootstraps will take the training data, further split it into a training and test set, then resample and repeat this process 25 times.\nset.seed(123) tidy_split \u0026lt;- initial_split(iris_df) tidy_split ## \u0026lt;Analysis/Assess/Total\u0026gt; ## \u0026lt;113/37/150\u0026gt; iris_train \u0026lt;- training(tidy_split) iris_test \u0026lt;- testing(tidy_split) iris_boots \u0026lt;- bootstraps(iris_train) iris_boots ## # Bootstrap sampling ## # A tibble: 25 x 2 ## splits id ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;split [113/39]\u0026gt; Bootstrap01 ## 2 \u0026lt;split [113/40]\u0026gt; Bootstrap02 ## 3 \u0026lt;split [113/44]\u0026gt; Bootstrap03 ## 4 \u0026lt;split [113/41]\u0026gt; Bootstrap04 ## 5 \u0026lt;split [113/39]\u0026gt; Bootstrap05 ## 6 \u0026lt;split [113/40]\u0026gt; Bootstrap06 ## 7 \u0026lt;split [113/42]\u0026gt; Bootstrap07 ## 8 \u0026lt;split [113/35]\u0026gt; Bootstrap08 ## 9 \u0026lt;split [113/42]\u0026gt; Bootstrap09 ## 10 \u0026lt;split [113/41]\u0026gt; Bootstrap10 ## # … with 15 more rows  Recipes Recipes is a powerful tool with functions for a wide range of feature engineering tasks designed to prepare data for modeling. Typing recipes:: into the Rstudio console is a great way to browse the available functions in the package.\nLet’s create a recipe to demonstrate how easy it is to apply feature engineering. There is really no need for feature engineering with this dataset, so we won’t actually use this recipe in the final workflow.\niris_rec \u0026lt;- recipe(species ~., data = iris_train) %\u0026gt;% step_pca(all_predictors()) %\u0026gt;% step_normalize(all_predictors()) prep \u0026lt;- prep(iris_rec) kable(head(iris_juice \u0026lt;- juice(prep)))   species PC1 PC2 PC3 PC4    setosa 1.1621621 1.384382 -0.0270717 0.0422303  setosa 1.3983644 1.159657 0.7767696 0.6840123  setosa 1.4836145 1.244589 0.0010748 0.1379599  setosa 1.4923559 1.089539 0.0331285 -0.4892659  setosa 1.1873971 1.402225 -0.4220083 -0.3874109  setosa 0.7736181 1.394953 -0.8518454 -0.1532614     Creating models with Parsnip Let’s set up two different models: first, a generalized linear model or glmnet. In this step we will create the model, workflow and fit the bootstraps. Let’s take a look at the output from each step.\n# set seed set.seed(1234) # generate the glmnet model with parsnip glmnet_mod \u0026lt;- multinom_reg(penalty = 0) %\u0026gt;% set_engine(\u0026quot;glmnet\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) glmnet_mod ## Multinomial Regression Model Specification (classification) ## ## Main Arguments: ## penalty = 0 ## ## Computational engine: glmnet # create a workflow glmnet_wf \u0026lt;- workflow() %\u0026gt;% add_formula(species ~ .) glmnet_wf ## ══ Workflow ════════════════════════════════════════════════════════════════════ ## Preprocessor: Formula ## Model: None ## ## ── Preprocessor ──────────────────────────────────────────────────────────────── ## species ~ . # add the model to the workflow and use iris_boots to fit our model 25 times glmnet_results \u0026lt;- glmnet_wf %\u0026gt;% add_model(glmnet_mod) %\u0026gt;% fit_resamples( resamples = iris_boots, control = control_resamples(extract = extract_model, save_pred = TRUE) ) glmnet_results ## # Resampling results ## # Bootstrap sampling ## # A tibble: 25 x 6 ## splits id .metrics .notes .extracts .predictions ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [39 ×… ## 2 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [40 ×… ## 3 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [44 ×… ## 4 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [41 ×… ## 5 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [39 ×… ## 6 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [40 ×… ## 7 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [42 ×… ## 8 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [35 ×… ## 9 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [42 ×… ## 10 \u0026lt;split [113… Bootstra… \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [1 … \u0026lt;tibble [41 ×… ## # … with 15 more rows # look at the model metrics collect_metrics(glmnet_results) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.944 25 0.00642 ## 2 roc_auc hand_till 0.987 25 0.00234 Now for a random forest model. We only need to tweak a few things and walah!\nset.seed(1234) rf_mod \u0026lt;- rand_forest() %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) # We set up a workflow and add the parts of our model together like legos rf_wf \u0026lt;- workflow() %\u0026gt;% add_formula(species ~ .) # Here we fit our 25 resampled datasets rf_results \u0026lt;- rf_wf %\u0026gt;% add_model(rf_mod) %\u0026gt;% fit_resamples( resamples = iris_boots, control = control_resamples(save_pred = TRUE) ) collect_metrics(rf_results) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.945 25 0.00764 ## 2 roc_auc hand_till 0.993 25 0.00126 Here’s a look at the confusion matrix summaries for both models. The confusion matrix let’s us see the correct and incorrect predictions of our models in a single table.\nglmnet_results %\u0026gt;% conf_mat_resampled()  ## # A tibble: 9 x 3 ## Prediction Truth Freq ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa setosa 13.3 ## 2 setosa versicolor 0 ## 3 setosa virginica 0 ## 4 versicolor setosa 0 ## 5 versicolor versicolor 12.2 ## 6 versicolor virginica 1 ## 7 virginica setosa 0 ## 8 virginica versicolor 1.24 ## 9 virginica virginica 12.4 rf_results %\u0026gt;% conf_mat_resampled()  ## # A tibble: 9 x 3 ## Prediction Truth Freq ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa setosa 13.3 ## 2 setosa versicolor 0 ## 3 setosa virginica 0 ## 4 versicolor setosa 0 ## 5 versicolor versicolor 12.3 ## 6 versicolor virginica 1.04 ## 7 virginica setosa 0 ## 8 virginica versicolor 1.16 ## 9 virginica virginica 12.4 The ROC curve helps us visually interpret our model performance at every threshold.\nglmnet_results %\u0026gt;% collect_predictions() %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(species, .pred_setosa:.pred_virginica) %\u0026gt;% autoplot() rf_results %\u0026gt;% collect_predictions() %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(species, .pred_setosa:.pred_virginica) %\u0026gt;% autoplot() + theme(legend.position = \u0026#39;none\u0026#39;)  Final fit This is it! By using the last_fit(tidy_split), we are able to train our model on the training set and test the model on the testing set in one fell swoop! Note, this is the only time we use the test set.\nfinal_glmnet \u0026lt;- glmnet_wf %\u0026gt;% add_model(glmnet_mod) %\u0026gt;% last_fit(tidy_split) final_glmnet ## # Resampling results ## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113… train/test … \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [37 × … \u0026lt;workflo… final_rf \u0026lt;- rf_wf %\u0026gt;% add_model(rf_mod) %\u0026gt;% last_fit(tidy_split) final_rf ## # Resampling results ## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples ## # A tibble: 1 x 6 ## splits id .metrics .notes .predictions .workflow ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; \u0026lt;list\u0026gt; ## 1 \u0026lt;split [113… train/test … \u0026lt;tibble [2 ×… \u0026lt;tibble [0 … \u0026lt;tibble [37 × … \u0026lt;workflo…  Confusion Matrices Finally, let’s generate a multiclass confusion matrix with the results from our test data. The confusion matrix provides a count of each outcome for all possible outcomes. The columns contain the true values and predictions are set across rows. This confusion matrix might look confusing because all predictions are correct.\ncollect_metrics(final_glmnet) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 1 ## 2 roc_auc hand_till 1 collect_predictions(final_glmnet) %\u0026gt;% conf_mat(species, .pred_class) %\u0026gt;% autoplot(type = \u0026#39;heatmap\u0026#39;)  collect_metrics(final_rf) ## # A tibble: 2 x 3 ## .metric .estimator .estimate ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.973 ## 2 roc_auc hand_till 1 collect_predictions(final_rf) %\u0026gt;% conf_mat(species, .pred_class) %\u0026gt;% autoplot(type = \u0026#39;heatmap\u0026#39;)  Final thoughts It’s always good to ask the question, ‘Do my results make sense’? Both models show near perfect predictive power but are they really that good? From our visual analysis, we can confidently say yes, there is a clear distinction between species when comparing the explanatory features. Now that we have a model, we have the power to predict iris type on new data 😄.\nI would like to thank the unsung heroes of the tidyverse and tidymodels: Julia Silge, David Robinson and Andrew Couch for creating and sharing amazing, open source learning resources.\n ","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603318262,"objectID":"517bb6adcf89c779300b0648b798dbfb","permalink":"/post/iris-classification/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/post/iris-classification/","section":"post","summary":"Flower classification using Tidymodels","tags":["machine learning","tidymodels"],"title":"Iris Classification","type":"post"},{"authors":[],"categories":[],"content":" Intro This post uses data from the  for Data Science online learning community. Each Tuesday, a dataset is posted and people share their analyses and visualizations on Twitter with the hashtag, #TidyTuesday. This week’s dataset (September 1st, 2020) is from Our World in Data. There are actually five datasets but we are going to focus on global crop yields.\n Download and view the data Let’s start by downloading the data from the R4DS Github account and take a look at it. the tidytuesdayR package (by Ellis Hughes, co-host of the screencast ‘TidyX’) is a handy package for checking out available datasets, exploring the data dictionary in the Rstudio viewer and loading the desired data (and more). Although simple to use, I’m going to stick with the readr::read_csv() from the tidyverse to import the data as a tibble (data frame).\n# Load the data with a readr function crop_yields \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv\u0026quot;) %\u0026gt;% janitor::clean_names() land_use \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/land_use_vs_yield_change_in_cereal_production.csv\u0026quot;) %\u0026gt;% janitor::clean_names() %\u0026gt;% select(entity, code, year, population = total_population_gapminder) %\u0026gt;% mutate(year = as.numeric(year)) crop_yield \u0026lt;- crop_yields %\u0026gt;% left_join(land_use, by = c(\u0026#39;code\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;entity\u0026#39;)) skimr::skim(crop_yields)  Table 1: Data summary  Name crop_yields  Number of rows 13075  Number of columns 14  _______________________   Column type frequency:   character 2  numeric 12  ________________________   Group variables None    Variable type: character\n  skim_variable n_missing complete_rate min max empty n_unique whitespace    entity 0 1.00 4 39 0 249 0  code 1919 0.85 3 8 0 214 0    Variable type: numeric\n  skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist    year 0 1.00 1990.37 16.73 1961.00 1976.00 1991.00 2005.00 2018.00 ▇▆▇▇▇  wheat_tonnes_per_hectare 4974 0.62 2.43 1.69 0.00 1.23 1.99 3.12 10.67 ▇▅▂▁▁  rice_tonnes_per_hectare 4604 0.65 3.16 1.85 0.20 1.77 2.74 4.16 10.68 ▇▇▃▁▁  maize_tonnes_per_hectare 2301 0.82 3.02 3.13 0.03 1.14 1.83 3.92 36.76 ▇▁▁▁▁  soybeans_tonnes_per_hectare 7114 0.46 1.45 0.75 0.00 0.86 1.33 1.90 5.95 ▇▇▂▁▁  potatoes_tonnes_per_hectare 3059 0.77 15.40 9.29 0.84 8.64 13.41 20.05 75.30 ▇▅▁▁▁  beans_tonnes_per_hectare 5066 0.61 1.09 0.82 0.03 0.59 0.83 1.35 9.18 ▇▁▁▁▁  peas_tonnes_per_hectare 6840 0.48 1.48 1.01 0.04 0.72 1.15 1.99 7.16 ▇▃▁▁▁  cassava_tonnes_per_hectare 5887 0.55 9.34 5.11 1.00 5.55 8.67 11.99 38.58 ▇▇▁▁▁  barley_tonnes_per_hectare 6342 0.51 2.23 1.50 0.09 1.05 1.88 3.02 9.15 ▇▆▂▁▁  cocoa_beans_tonnes_per_hectare 8466 0.35 0.39 0.28 0.00 0.24 0.36 0.49 3.43 ▇▁▁▁▁  bananas_tonnes_per_hectare 4166 0.68 15.20 12.08 0.66 5.94 11.78 20.79 77.59 ▇▃▁▁▁    # Here are a few other options for getting to know the data # glimpse(crop_yields) # summary(crop_yields) # View(crop_yields)  Tidy data: cleaning and reshaping Let’s start with a little data cleaning! skim() from skimr provides a convenient summary. By looking at the data we can see that there are missing values that need to be removed or imputed but other than that, the data is clean. One thing we should consider is whether the data is in a tidy format where each column is a feature and each row a sample. This long format facilitates fluent analysis with the tidyverse. The function pivot_longer() from the tidyr package allows us to reshape the data and prepare it for exploratory data analysis (EDA).\n# Use pivot_longer from tidyr to tidy the data. crop_yield_tidy \u0026lt;- crop_yield %\u0026gt;% rename_all(str_remove, \u0026quot;_tonnes.*\u0026quot;) %\u0026gt;% rename(country = entity) %\u0026gt;% pivot_longer(wheat:bananas, names_to = \u0026#39;crop\u0026#39;, values_to = \u0026#39;yield_hectare\u0026#39;) %\u0026gt;% drop_na(yield_hectare) # Create a vector of the 25 countries with the largest population top_pops \u0026lt;- land_use %\u0026gt;% filter(!is.na(code), entity != \u0026quot;World\u0026quot;) %\u0026gt;% group_by(entity) %\u0026gt;% filter(year == max(year)) %\u0026gt;% ungroup() %\u0026gt;% slice_max(population, n = 25) %\u0026gt;% pull(entity) kable(head(crop_yield_tidy, n = 10))   country code year population crop yield_hectare    Afghanistan AFG 1961 9169000 wheat 1.0220  Afghanistan AFG 1961 9169000 rice 1.5190  Afghanistan AFG 1961 9169000 maize 1.4000  Afghanistan AFG 1961 9169000 potatoes 8.6667  Afghanistan AFG 1961 9169000 barley 1.0800  Afghanistan AFG 1962 9351000 wheat 0.9735  Afghanistan AFG 1962 9351000 rice 1.5190  Afghanistan AFG 1962 9351000 maize 1.4000  Afghanistan AFG 1962 9351000 potatoes 7.6667  Afghanistan AFG 1962 9351000 barley 1.0800     What can we learn from this dataset? A great place to start any analysis is by grouping and counting predictive features using the count() function. It often makes sense to combine count() with a bar plot to visualize meaningful distributions. EDA is arguably the most important step in a machine learning workflow because it gives us insight that we can then use to generate a predictive model. When we run a machine learning model, we should already have an intutive sense about what to expect from our model.\ncrop_yield_tidy %\u0026gt;% add_count(year, country, sort = T) %\u0026gt;% filter(country %in% c(\u0026quot;Albania\u0026quot;, \u0026quot;Africa\u0026quot;, \u0026quot;United States\u0026quot;)) # Africa has 11 crops, US has 9 and Albania 6.  ## # A tibble: 1,522 x 7 ## country code year population crop yield_hectare n ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 Africa \u0026lt;NA\u0026gt; 1961 290214016 wheat 0.693 11 ## 2 Africa \u0026lt;NA\u0026gt; 1961 290214016 rice 1.55 11 ## 3 Africa \u0026lt;NA\u0026gt; 1961 290214016 maize 1.04 11 ## 4 Africa \u0026lt;NA\u0026gt; 1961 290214016 soybeans 0.376 11 ## 5 Africa \u0026lt;NA\u0026gt; 1961 290214016 potatoes 8.18 11 ## 6 Africa \u0026lt;NA\u0026gt; 1961 290214016 beans 0.587 11 ## 7 Africa \u0026lt;NA\u0026gt; 1961 290214016 peas 0.679 11 ## 8 Africa \u0026lt;NA\u0026gt; 1961 290214016 cassava 5.66 11 ## 9 Africa \u0026lt;NA\u0026gt; 1961 290214016 barley 0.446 11 ## 10 Africa \u0026lt;NA\u0026gt; 1961 290214016 cocoa_beans 0.254 11 ## # … with 1,512 more rows crop_yield_tidy %\u0026gt;% count(country, sort = TRUE) %\u0026gt;% filter(n \u0026gt;=500) # this gives a list of the countries with the most years + crops 11 (crops) * 51 (year) = 561 ## # A tibble: 53 x 2 ## country n ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Africa 638 ## 2 Americas 638 ## 3 Asia 638 ## 4 Central America 638 ## 5 Democratic Republic of Congo 638 ## 6 Eastern Africa 638 ## 7 Ecuador 638 ## 8 Land Locked Developing Countries 638 ## 9 Least Developed Countries 638 ## 10 Low Income Food Deficit Countries 638 ## # … with 43 more rows crop_yield_tidy %\u0026gt;% summarize(max(year)- min(year)) #57 years of recorded data ## # A tibble: 1 x 1 ## `max(year) - min(year)` ## \u0026lt;dbl\u0026gt; ## 1 57 crop_yield_tidy %\u0026gt;% # 249 unique countries distinct(country) %\u0026gt;% nrow() ## [1] 249  Visual analysis Now the data is tidy, we are ready to take a closer look at it. Our initial EDA should have given us some ideas about what relationships we might want to pursue with visual analysis. Let’s start by comparing crop yield by country. There are too many countries to add to a single plot, so we might need to plot a subset of the data. For more refined control of country selection, we c code into a Shiny dashboard.\nWe will use slice_sample() to randomly select a subset of the total countries. Unless a seed is set, the selection will be different each time we run the code chunk. Let’s choose 9 countries to get a feel for crop yields over time without overwhelming our senses.\n# Randomly select 9 country names # set.seed(1014) if you want to select the same 9 countries each time crops \u0026lt;- crop_yield_tidy %\u0026gt;% distinct(crop) %\u0026gt;% pull() country_random \u0026lt;- crop_yield_tidy %\u0026gt;% select(country) %\u0026gt;% distinct() %\u0026gt;% slice_sample(n = 9) %\u0026gt;% pull() crop_yield_tidy %\u0026gt;% filter(country == \u0026#39;United States\u0026#39;) %\u0026gt;% ggplot(aes(year, yield_hectare, color = crop)) + geom_line() + facet_wrap(~country, \u0026#39;free_x\u0026#39;) + theme(plot.title.position = \u0026#39;plot\u0026#39;) + labs( title = \u0026#39;Crop Yield USA\u0026#39;, y = \u0026#39;Year\u0026#39;, x = \u0026#39;tonnes per hectare per year\u0026#39; ) # plot of USA data only crop_yield_tidy %\u0026gt;% filter(country == \u0026#39;United States\u0026#39;) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(yield_hectare, crop, fill = crop)) + geom_col() + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;, plot.title.position = \u0026#39;plot\u0026#39;) + labs( title = \u0026#39;Crop Yield USA\u0026#39;, y = \u0026#39;Crop\u0026#39;, x = \u0026#39;tonnes per hectare\u0026#39; ) # Plot crop yield per hectare per year. crop_yield_tidy %\u0026gt;% filter(country %in% country_random, crop %in% c(\u0026#39;maize\u0026#39;, \u0026#39;potatoes\u0026#39;, \u0026#39;wheat\u0026#39;, \u0026#39;bananas\u0026#39;)) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(year, yield_hectare, color = crop)) + geom_line(size = 1, alpha = 0.5) + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + theme(plot.title.position = \u0026#39;plot\u0026#39;) + labs( y = \u0026#39;yield per hectare\u0026#39;, title = \u0026#39;Crop Yield\u0026#39; ) crop_yield_tidy %\u0026gt;% filter(country %in% top_pops, crop == c(\u0026#39;maize\u0026#39;, \u0026#39;potatoes\u0026#39;, \u0026#39;wheat\u0026#39;, \u0026#39;bananas\u0026#39;)) %\u0026gt;% mutate(crop = reorder_within(crop, yield_hectare, country)) %\u0026gt;% ggplot(aes(yield_hectare, crop, fill = crop)) + geom_col() + scale_y_reordered() + facet_wrap(~country, scales = \u0026#39;free\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;, plot.title.position = \u0026#39;plot\u0026#39;) + labs( title = \u0026#39;Crop Yield\u0026#39;, y = \u0026#39;Crop\u0026#39;, x = \u0026#39;yield per hectare\u0026#39; ) crop_yield_tidy %\u0026gt;% filter(country %in% top_pops, crop %in% c(\u0026#39;maize\u0026#39;, \u0026#39;potatoes\u0026#39;, \u0026#39;wheat\u0026#39;, \u0026#39;bananas\u0026#39;)) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(year, yield_hectare, color = crop)) + geom_line(size = 1, alpha = 0.5) + theme(plot.title.position = \u0026#39;plot\u0026#39;) + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + labs( y = \u0026#39;yield per hectare\u0026#39;, title = \u0026#39;Crop Yield\u0026#39; ) Next, let’s make a regression model for each of the top 20 countries.\nlibrary(tidymodels) library(ggrepel) crop_yield_tidy %\u0026gt;% drop_na() %\u0026gt;% filter(country %in% top_pops, crop %in% c(\u0026#39;maize\u0026#39;, \u0026#39;potatoes\u0026#39;, \u0026#39;wheat\u0026#39;, \u0026#39;bananas\u0026#39;)) %\u0026gt;% nest(mod_data = c(year, yield_hectare, population)) %\u0026gt;% mutate(model = map(mod_data, ~lm(yield_hectare ~ population + year, data = .))) %\u0026gt;% mutate(coefs = map(model, tidy)) %\u0026gt;% unnest(coefs) %\u0026gt;% filter(term == \u0026quot;year\u0026quot;) %\u0026gt;% mutate(p.value = p.adjust(p.value)) %\u0026gt;% ggplot(aes(estimate, p.value, label = country)) + geom_vline(xintercept = 0, lty = 2, size = 1.5, alpha = 0.7, color = \u0026quot;gray50\u0026quot;) + geom_point(aes(color = crop), alpha = 0.8, size = 2.5, show.legend = FALSE) + scale_y_log10() + facet_wrap(~crop) + geom_text_repel(size = 2.5, family = \u0026quot;Times New Roman\u0026quot;)   ","date":1599091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"7ab900960e472a8bde7aa822b075d5b9","permalink":"/post/hello-tidyverse/","publishdate":"2020-09-03T00:00:00Z","relpermalink":"/post/hello-tidyverse/","section":"post","summary":"This post explores crop yield data with visualization and the lm() function to generate many regression models (a model for each country (25) and crop (4).","tags":[],"title":"Hello Tidyverse and generating many models","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]