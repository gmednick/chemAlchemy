[{"authors":["gabe"],"categories":null,"content":"My journey into the physical and biological sciences started with a desire to study osteopathic medicine. In the process of completing a biochemistry and molecular biology degree, my interest in the structure and function of the human body grew into a fascination with the invisible structure and function of the cell. I developed a deep interest in both physical chemistry and biochemistry, and my curiosity resulted in a PhD focussed on sensory transduction pathways and light sensing mechanisms in bacteria. After finishing my PhD, I developed and implemented innovative teaching practices in chemistry and biology at the university level. More recently (2018-2020), I worked as a senior scientist for a small startup with an emphasis on DNA and RNA synthesis. I love hands-on research but my current professional passions are data science and bioinformatics. Data informed choices can provide insight, drive innovation and optimize decision making. My mission is to facilitate this process.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d5326a2f1cf09d7e5a9f14131238744","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"My journey into the physical and biological sciences started with a desire to study osteopathic medicine. In the process of completing a biochemistry and molecular biology degree, my interest in the structure and function of the human body grew into a fascination with the invisible structure and function of the cell.","tags":null,"title":"","type":"authors"},{"authors":[],"categories":[],"content":" The iris dataset is a built in dataset in R. Use data() to see available datasets (many packages also come with their own datasets).\n# load the iris data set iris_df\u0026lt;- iris %\u0026gt;% clean_names() # iris_df %\u0026gt;% View() # equal number of each species, 150 total iris_df %\u0026gt;% count(species) ## species n ## 1 setosa 50 ## 2 versicolor 50 ## 3 virginica 50 # no NAs iris_df %\u0026gt;% summary() ## sepal_length sepal_width petal_length petal_width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ##  Visualize relationships Before we do any kind of machine learning, let’s visualize the features and start thinking about what kind of questions we can ask. And ‘see’ how our features correlate which will also tell us about how well we will be able to make predictions with our data.\niris_df %\u0026gt;% ggplot(aes(sepal_length, sepal_width, color = species)) + geom_point() + facet_wrap(~species) iris_df %\u0026gt;% ggplot(aes(petal_length, petal_width, color = species)) + geom_point() + facet_wrap(~species) # Tidy format Let’s change the shape of our data by combining all iris features into a single category called metric and the associated values will go into a column that we name value.\niris_df_long \u0026lt;- iris_df %\u0026gt;% pivot_longer(cols = sepal_length:petal_width, names_to = \u0026#39;metric\u0026#39;, values_to =\u0026#39;value\u0026#39;) iris_df_long %\u0026gt;% ggplot(aes(value, metric, color = species)) + geom_boxplot() + coord_flip() iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_histogram(bins = 20, alpha = 0.7) + facet_wrap(~ metric, scales = \u0026quot;free_x\u0026quot;) iris_df_long %\u0026gt;% ggplot(aes(value, fill = species)) + geom_density(alpha = .5) + facet_wrap(~ metric, scales = \u0026quot;free\u0026quot;) iris_df_long %\u0026gt;% ggplot(aes(species, value, color = species)) + geom_boxplot(alpha = 0.3) + facet_wrap(~ metric, scales = \u0026quot;free_y\u0026quot;) Let’s get modeling! First, we split the data. Since we only have 150 samples (50 for each iris species), we will also use bootstrap resampling. set.seed(123) tidy_split \u0026lt;- initial_split(iris_df) iris_train \u0026lt;- training(tidy_split) iris_test \u0026lt;- testing(tidy_split) iris_boots \u0026lt;- bootstraps(iris_train)   Recipes is the way to go for feature engineering. A great way to see the available functions in Rstudio: type recipes:: into the console and scroll. iris_rec \u0026lt;- recipe(species ~ ., data = iris_train) %\u0026gt;% step_zv(all_predictors()) %\u0026gt;% step_normalize(all_predictors()) prep \u0026lt;- prep(iris_rec) kable(head(iris_juice \u0026lt;- juice(prep)))   sepal_length sepal_width petal_length petal_width species    -0.9406570 0.9308067 -1.330437 -1.280818 setosa  -1.2040875 -0.1535351 -1.330437 -1.280818 setosa  -1.4675181 0.2802016 -1.388618 -1.280818 setosa  -1.5992334 0.0633332 -1.272256 -1.280818 setosa  -1.0723722 1.1476751 -1.330437 -1.280818 setosa  -0.5455111 1.7982802 -1.155894 -1.018146 setosa     Create models. Let’s set up two different models: logistic regression and random forest\nglm_spec \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) glm_wf \u0026lt;- workflow() %\u0026gt;% add_recipe(iris_rec) %\u0026gt;% add_model(glm_spec) rf_spec \u0026lt;- rand_forest(trees = 1000) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) rf_wf \u0026lt;- workflow() %\u0026gt;% add_recipe(iris_rec) %\u0026gt;% add_model(rf_spec) set.seed(1234) glm_rs \u0026lt;- glm_wf %\u0026gt;% fit_resamples( resamples = iris_boots, metrics = metric_set(accuracy, sens), control = control_grid(save_pred = TRUE) ) ## ! Bootstrap01: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap02: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap03: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap04: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap05: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap06: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap07: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap08: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap09: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap10: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap11: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap12: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap13: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap14: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap15: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap16: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap17: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap18: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap19: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap20: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap21: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap22: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap23: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap24: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... ## ! Bootstrap25: model: glm.fit: algorithm did not converge, glm.fit: fitted probabilitie... collect_metrics(glm_rs) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.667 25 0.0126 ## 2 sens macro 0.667 25 0 rf_rs \u0026lt;- rf_wf %\u0026gt;% fit_resamples( resamples = iris_boots, metrics = metric_set(accuracy, sens), control = control_grid(save_pred = TRUE) ) collect_metrics(rf_rs) ## # A tibble: 2 x 5 ## .metric .estimator mean n std_err ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy multiclass 0.939 25 0.00778 ## 2 sens macro 0.943 25 0.00728 # glm_rs %\u0026gt;% # collect_predictions() %\u0026gt;% # group_by(id) %\u0026gt;% # roc_curve(species, .pred_class) %\u0026gt;% # ggplot(aes(1 - specificity, sensitivity, color = id)) + # geom_abline(lty = 2, color = \u0026quot;gray80\u0026quot;, size = 1.5) + # geom_path(show.legend = FALSE, alpha = 0.6, size = 1.2) + # coord_equal()   ","date":1603238400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603318262,"objectID":"517bb6adcf89c779300b0648b798dbfb","permalink":"/post/iris-classification/","publishdate":"2020-10-21T00:00:00Z","relpermalink":"/post/iris-classification/","section":"post","summary":"Flower classification using Tidymodels","tags":["machine learning","tidymodels"],"title":"Iris Classification","type":"post"},{"authors":[],"categories":[],"content":" Intro This data comes from the R4DS TidyTuesday project. There are five related datasets but we will only look at key_crop_yields dataset in this exploration. The data contains crop yield (tonnes per hectare) by country and year. Let’s get started by loading the packages that we will need for this analysis.\nNext up, let’s download the data from the R4DS Github account and take a look at it. There is the wonderful package, tidytuesdayR by Ellis Hughes, that allows one to load and explore the TidyTuesday data in the Rstudio viewer. We will use the readr package from the tidyverse using read_csv to import our data as a modern data frame or tibble.\n# Load the data with a readr function crop_yields \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv\u0026quot;) # land_use \u0026lt;- read_csv(\u0026quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/land_use_vs_yield_change_in_cereal_production.csv\u0026quot;) %\u0026gt;% janitor::clean_names() # The skim function provides a great way to get an initial summary. I typically use View() as well to view the dataset in Rstudio skimr::skim(crop_yields)  Table 1: Data summary  Name crop_yields  Number of rows 13075  Number of columns 14  _______________________   Column type frequency:   character 2  numeric 12  ________________________   Group variables None    Variable type: character\n  skim_variable n_missing complete_rate min max empty n_unique whitespace    Entity 0 1.00 4 39 0 249 0  Code 1919 0.85 3 8 0 214 0    Variable type: numeric\n    skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist    Year 0 1.00 1990.37 16.73 1961.00 1976.00 1991.00 2005.00 2018.00 ▇▆▇▇▇  Wheat (tonnes per hectare) 4974 0.62 2.43 1.69 0.00 1.23 1.99 3.12 10.67 ▇▅▂▁▁  Rice (tonnes per hectare) 4604 0.65 3.16 1.85 0.20 1.77 2.74 4.16 10.68 ▇▇▃▁▁  Maize (tonnes per hectare) 2301 0.82 3.02 3.13 0.03 1.14 1.83 3.92 36.76 ▇▁▁▁▁  Soybeans (tonnes per hectare) 7114 0.46 1.45 0.75 0.00 0.86 1.33 1.90 5.95 ▇▇▂▁▁  Potatoes (tonnes per hectare) 3059 0.77 15.40 9.29 0.84 8.64 13.41 20.05 75.30 ▇▅▁▁▁  Beans (tonnes per hectare) 5066 0.61 1.09 0.82 0.03 0.59 0.83 1.35 9.18 ▇▁▁▁▁  Peas (tonnes per hectare) 6840 0.48 1.48 1.01 0.04 0.72 1.15 1.99 7.16 ▇▃▁▁▁  Cassava (tonnes per hectare) 5887 0.55 9.34 5.11 1.00 5.55 8.67 11.99 38.58 ▇▇▁▁▁  Barley (tonnes per hectare) 6342 0.51 2.23 1.50 0.09 1.05 1.88 3.02 9.15 ▇▆▂▁▁  Cocoa beans (tonnes per hectare) 8466 0.35 0.39 0.28 0.00 0.24 0.36 0.49 3.43 ▇▁▁▁▁  Bananas (tonnes per hectare) 4166 0.68 15.20 12.08 0.66 5.94 11.78 20.79 77.59 ▇▃▁▁▁    # Here are a few other options for getting to know our data # glimpse(crop_yields) # summary(crop_yields) # View(crop_yields)  Data cleaning and reshaping Let’s start with a little data cleaning! Using skim from skimr provides a convenient summary. We can see that there are plenty of NAs that we will most likely need to remove. We can also standardize our feature names with the janitor package. Lastly, we should consider the shape of our data and make sure that it’s in a tidy format where each column is a feature and each row a sample.This long or tidy format is a prerequisite for fluent analysis with the tidyverse and tidymodels frameworks. In our data example, the various crops on samples, that can be categorized as the feature crops. The function pivot_longer is fantastic function from the tidyr package that allows us to reshape our data and preparing it for This will benefit our downstream analysis, including EDA (exploratory data analysis) and modeling (machine learning).\ncrop_yields %\u0026gt;% count(Year) ## # A tibble: 58 x 2 ## Year n ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1961 209 ## 2 1962 209 ## 3 1963 209 ## 4 1964 209 ## 5 1965 209 ## 6 1966 211 ## 7 1967 211 ## 8 1968 213 ## 9 1969 213 ## 10 1970 213 ## # … with 48 more rows # Use janitor package to clean column names and pivot_longer from tidyr to tidy the data. crop_yield_tidy \u0026lt;- crop_yields %\u0026gt;% janitor::clean_names() %\u0026gt;% rename_all(str_remove, \u0026quot;_tonnes.*\u0026quot;) %\u0026gt;% rename(country = entity) %\u0026gt;% pivot_longer(wheat:bananas, names_to = \u0026#39;crop\u0026#39;, values_to = \u0026#39;yield_hectare\u0026#39;) %\u0026gt;% drop_na(yield_hectare) kable(head(crop_yield_tidy, n = 10))   country code year crop yield_hectare    Afghanistan AFG 1961 wheat 1.0220  Afghanistan AFG 1961 rice 1.5190  Afghanistan AFG 1961 maize 1.4000  Afghanistan AFG 1961 potatoes 8.6667  Afghanistan AFG 1961 barley 1.0800  Afghanistan AFG 1962 wheat 0.9735  Afghanistan AFG 1962 rice 1.5190  Afghanistan AFG 1962 maize 1.4000  Afghanistan AFG 1962 potatoes 7.6667  Afghanistan AFG 1962 barley 1.0800    # crop_yield_tidy %\u0026gt;% # summary() # # crop_yield_tidy %\u0026gt;% # count(year, country, sort = TRUE) # # tidy_select \u0026lt;- crop_yield_tidy %\u0026gt;% # count(country, sort = TRUE) %\u0026gt;% # filter(n \u0026gt;=500) # # crop_yield_tidy %\u0026gt;% # summarize(max(year)- min(year)) #57 years of recorded data # # crop_yield_tidy %\u0026gt;% # count(year, crop, sort = T) # # crop_yield_tidy %\u0026gt;% # filter(country == \u0026#39;United States\u0026#39;) %\u0026gt;% # count(crop)  Visual analysis Now that we have the data in tidy format, let’s see what we can learn from it. We want to look at crop production by country but there are too many countries to look at in a single plot. slice_sample() can be used to randomly select a subset of the total countries. We will use the results to visualize a subset of the countries with the idea that this will help us get a feel for relationships in the data. For more refined control of the country selection, we could incorporate the code into a Shiny dashboard with user conntrolled selection inputs.\n# Randomly select 9 country names # set.seed(1014) if you want the same 9 countries to be selected each time country_random \u0026lt;- crop_yield_tidy %\u0026gt;% select(country) %\u0026gt;% distinct() %\u0026gt;% slice_sample(n = 9) %\u0026gt;% pull() # another option would be to set the code equal to a set selection of countries: E.g., countries \u0026lt;- c(\u0026#39;GUM\u0026#39;, \u0026#39;IND\u0026#39;, \u0026#39;YEM\u0026#39;, \u0026#39;NAM\u0026#39;, \u0026#39;URY\u0026#39;, \u0026#39;USA\u0026#39;) # Plot crop yield per hectare. library(tidytext) usa_col_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(country == \u0026#39;United States\u0026#39;) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(yield_hectare, crop, fill = crop)) + geom_col() + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) + labs( title = \u0026#39;Crop Yield USA\u0026#39;, y = \u0026#39;Crop\u0026#39;, x = \u0026#39;tonnes per hectare\u0026#39; ) usa_col_plot column_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(country == country_random) %\u0026gt;% mutate(crop = reorder_within(crop, yield_hectare, country)) %\u0026gt;% ggplot(aes(yield_hectare, crop, fill = crop)) + geom_col() + scale_y_reordered() + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;) + labs( title = \u0026#39;Crop Yield\u0026#39;, y = \u0026#39;Crop\u0026#39;, x = \u0026#39;yield per hectare\u0026#39; ) column_plot Figure 1. This column plot shows the crop on the y-axis, yield on the x-axis and is faceted by country.\n# Plot crop yield per hectare per year. line_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(year \u0026gt;= 2000, country == country_random) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(year, yield_hectare, color = crop)) + geom_line() + facet_wrap(~country, scales = \u0026#39;free_y\u0026#39;) + labs( y = \u0026#39;yield per hectare\u0026#39;, title = \u0026#39;Crop Yield\u0026#39; ) line_plot # Plot crop yield per hectare per year. box_plot \u0026lt;- crop_yield_tidy %\u0026gt;% filter(year \u0026gt;= 2000, country == country_random) %\u0026gt;% mutate(crop = fct_reorder(crop, yield_hectare)) %\u0026gt;% ggplot(aes(yield_hectare, crop, color = crop)) + geom_boxplot() + theme(legend.position = \u0026#39;none\u0026#39;) + facet_wrap(~country, scales = \u0026#39;free\u0026#39;) + labs( x = \u0026#39;Yield\u0026#39;, title = \u0026#39;Crop Yield\u0026#39;, y = \u0026#39;\u0026#39; ) box_plot  ","date":1599091200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603152000,"objectID":"7ab900960e472a8bde7aa822b075d5b9","permalink":"/post/hello-tidyverse/","publishdate":"2020-09-03T00:00:00Z","relpermalink":"/post/hello-tidyverse/","section":"post","summary":"Data wrangling and visualization: insights into data analysis with the tidyverse","tags":[],"title":"Hello Tidyverse and Tidymodels","type":"post"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]