<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning | Gabe Mednick</title>
    <link>/tag/machine-learning/</link>
      <atom:link href="/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>machine learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Gabe Mednick 2020</copyright><lastBuildDate>Wed, 21 Oct 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>machine learning</title>
      <link>/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Iris Classification</title>
      <link>/post/iris-classification/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/iris-classification/</guid>
      <description>


&lt;div id=&#34;the-iris-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The iris dataset&lt;/h1&gt;
&lt;p&gt;The iris dataset is a classic, so much so that it comes packaged with base R. You can use the &lt;code&gt;data()&lt;/code&gt; function to see a list of datasets that come with R. There are often datasets associated with packages as well. Try &lt;code&gt;data(package = &#39;dplyr&#39;)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s take a look at the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the iris data set and clean the column names with janitor::clean_names()
iris_df&amp;lt;- iris %&amp;gt;% 
  clean_names() 

iris_df %&amp;gt;%  head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   sepal_length sepal_width petal_length petal_width species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris_df %&amp;gt;%  count(species)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      species  n
## 1     setosa 50
## 2 versicolor 50
## 3  virginica 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# equal number of each species, 150 total

iris_df %&amp;gt;%  str()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    150 obs. of  5 variables:
##  $ sepal_length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ sepal_width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ petal_length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ petal_width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ species     : Factor w/ 3 levels &amp;quot;setosa&amp;quot;,&amp;quot;versicolor&amp;quot;,..: 1 1 1 1 1 1 1 1 1 1 ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset is clean, but with only 150 observations, we must question whether it’s too small to train a reliable model. We will generate bootstrap resamples to make the most of what we have.&lt;/p&gt;
&lt;p&gt;We see that we have four features (sepal length and with, and petal length and width) and there are three unique species..&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It makes sense to create a model that predicts the species of iris based on the flower’s measurements.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-relationships&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Visualize relationships&lt;/h1&gt;
&lt;p&gt;Before we do any kind of machine learning, let’s visualize the relationships in our data and get a feel for the predictive power of our features. We can do this by plotting the explanatory features and then use color or faceting to create the distinction of species.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sepal &amp;lt;- iris_df %&amp;gt;% 
  ggplot(aes(sepal_length, sepal_width, color = species)) +
  geom_point(size = 1) + facet_wrap(~species) +
  labs(x = &amp;#39;sepal length&amp;#39;,
       y = &amp;#39;sepal width&amp;#39;)

petal &amp;lt;- iris_df %&amp;gt;% 
  ggplot(aes(petal_length, petal_width, color = species)) +
  geom_point(size =1) + facet_wrap(~species) +
  labs(x = &amp;#39;petal length&amp;#39;,
       y = &amp;#39;petal width&amp;#39;)

(petal/sepal) # patchwork allows us to arrange plots side-by-side or stacked but there&amp;#39;s a better way..&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy-the-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Tidy the dataset&lt;/h1&gt;
&lt;p&gt;Let’s change the shape of our data by combining the four iris features into a single column (we’ll name, e.g., &lt;code&gt;metric&lt;/code&gt;) and the associated values populate a new column (named &lt;code&gt;value&lt;/code&gt;). This transformation into a longer dataset can be done with the function &lt;code&gt;pivot_longer()&lt;/code&gt;. As we shall see, the tidy format lends itself to data visualization and analysis with the tidyverse.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris_df_long &amp;lt;- iris_df %&amp;gt;%  
  pivot_longer(cols = sepal_length:petal_width,
               names_to = &amp;#39;metric&amp;#39;,
               values_to =&amp;#39;value&amp;#39;) 
# A boxplot is a great way to see the median values of our features by species.
iris_df_long %&amp;gt;% 
  ggplot(aes(metric, value, color = species)) +
  geom_boxplot() &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# A nice alternative is to facet by the metric to convey the same information but with added clarity. 
iris_df_long %&amp;gt;%
  ggplot(aes(species, value, color = species)) +
  geom_boxplot(alpha = 0.3) +
  facet_wrap(~ metric, scales = &amp;quot;free_y&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The same information can be displayed by comparing the distributions in histogram.
iris_df_long %&amp;gt;%
  ggplot(aes(value, fill = species)) +
  geom_histogram(bins = 20, alpha = 0.7) +
  facet_wrap(~ metric, scales = &amp;quot;free_x&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# geom_denisty is a nice alternative to geom_histogram.
iris_df_long %&amp;gt;% 
  ggplot(aes(value, fill = species)) +
  geom_density(alpha = .5) +
  facet_wrap(~ metric, scales = &amp;quot;free&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-3-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;got-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Got models&lt;/h1&gt;
&lt;p&gt;Before we get to modeling, we will want to split the data and typically some feature engineering may be necessary as well. Since our dataset is small, we are going to take the training set and make 25 bootstrap resamples. By default, &lt;code&gt;initial split&lt;/code&gt; provides a 75:25 split for our train and test sets respectively. The function &lt;code&gt;bootstraps&lt;/code&gt; will take the training data, further split it into a training and test set, then resample and repeat 25 times. This ‘resampling’ provides a more robust method to test our model validity.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
tidy_split &amp;lt;- initial_split(iris_df)
tidy_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;Analysis/Assess/Total&amp;gt;
## &amp;lt;113/37/150&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris_train &amp;lt;- training(tidy_split)
iris_test &amp;lt;- testing(tidy_split)

iris_boots &amp;lt;- bootstraps(iris_train) 
iris_boots&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Bootstrap sampling 
## # A tibble: 25 x 2
##    splits           id         
##    &amp;lt;list&amp;gt;           &amp;lt;chr&amp;gt;      
##  1 &amp;lt;split [113/39]&amp;gt; Bootstrap01
##  2 &amp;lt;split [113/40]&amp;gt; Bootstrap02
##  3 &amp;lt;split [113/44]&amp;gt; Bootstrap03
##  4 &amp;lt;split [113/41]&amp;gt; Bootstrap04
##  5 &amp;lt;split [113/39]&amp;gt; Bootstrap05
##  6 &amp;lt;split [113/40]&amp;gt; Bootstrap06
##  7 &amp;lt;split [113/42]&amp;gt; Bootstrap07
##  8 &amp;lt;split [113/35]&amp;gt; Bootstrap08
##  9 &amp;lt;split [113/42]&amp;gt; Bootstrap09
## 10 &amp;lt;split [113/41]&amp;gt; Bootstrap10
## # … with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;recipes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recipes&lt;/h1&gt;
&lt;p&gt;Recipes is a powerful tool for a wide range of feature engineering tasks that can be prepare your data for modeling. A great way to see the available functions in a package is to type the package name and two colons, eg &lt;code&gt;recipes::&lt;/code&gt; into the Rstudio console. All package functions will pop up and a brief description for the highlighted function will be available.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;recipes_functions.jpg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s create a recipe to demonstrate how easy it is to apply feature engineering. Since the features of the iris dataset do not actually require any feature engineering, we won’t include this recipe in our final workflow.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;iris_rec &amp;lt;- recipe(species ~., data = iris_train) %&amp;gt;%
  step_pca(all_predictors()) %&amp;gt;%
  step_normalize(all_predictors())

prep &amp;lt;-  prep(iris_rec)

kable(head(iris_juice &amp;lt;- juice(prep)))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;species&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;PC1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;PC2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;PC3&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;PC4&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1621621&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.384382&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0270717&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0422303&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3983644&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.159657&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7767696&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6840123&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4836145&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.244589&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0010748&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1379599&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.4923559&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.089539&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0331285&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4892659&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1873971&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.402225&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4220083&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3874109&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;setosa&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7736181&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.394953&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.8518454&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.1532614&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-models-with-the-recipes-package&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Creating models with the Recipes package&lt;/h1&gt;
&lt;p&gt;Let’s set up two different models: first up, the &lt;strong&gt;generalized linear model&lt;/strong&gt; or &lt;strong&gt;glmnet&lt;/strong&gt;. I will run each part of the code to preview the internals of the model as we go .&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set seed
set.seed(1234)

# generate the glmnet model with parsnip
glmnet_mod &amp;lt;- multinom_reg(penalty = 0) %&amp;gt;% 
  set_engine(&amp;quot;glmnet&amp;quot;) %&amp;gt;% 
  set_mode(&amp;quot;classification&amp;quot;)
glmnet_mod&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = 0
## 
## Computational engine: glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create a workflow
glmnet_wf &amp;lt;- workflow() %&amp;gt;%
  add_formula(species ~ .) 
glmnet_wf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ══ Workflow ═══════════════════════════════════
## Preprocessor: Formula
## Model: None
## 
## ── Preprocessor ───────────────────────────────
## species ~ .&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add the model to the workflow and use iris_boots to fit our model 25 times
glmnet_results &amp;lt;- glmnet_wf %&amp;gt;%
  add_model(glmnet_mod) %&amp;gt;% 
  fit_resamples(
    resamples = iris_boots,
    control = control_resamples(extract = extract_model,
                             save_pred = TRUE)
    )
glmnet_results&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Resampling results
## # Bootstrap sampling 
## # A tibble: 25 x 6
##    splits       id        .metrics      .notes       .extracts    .predictions  
##    &amp;lt;list&amp;gt;       &amp;lt;chr&amp;gt;     &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;        
##  1 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [39 ×…
##  2 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [40 ×…
##  3 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [44 ×…
##  4 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [41 ×…
##  5 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [39 ×…
##  6 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [40 ×…
##  7 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [42 ×…
##  8 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [35 ×…
##  9 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [42 ×…
## 10 &amp;lt;split [113… Bootstra… &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [1 … &amp;lt;tibble [41 ×…
## # … with 15 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# look at the model metrics
collect_metrics(glmnet_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 accuracy multiclass 0.944    25 0.00642
## 2 roc_auc  hand_till  0.987    25 0.00234&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s repeat the same process but switch out the model. I will choose a &lt;strong&gt;random forest&lt;/strong&gt; model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1234)
rf_mod &amp;lt;- rand_forest() %&amp;gt;%
  set_engine(&amp;quot;ranger&amp;quot;) %&amp;gt;%
  set_mode(&amp;quot;classification&amp;quot;)

# We set up a workflow and add the parts of our model together like legos
rf_wf &amp;lt;- workflow() %&amp;gt;%
  add_formula(species ~ .)

# Here we fit our 25 resampled datasets 
rf_results &amp;lt;- rf_wf %&amp;gt;%
  add_model(rf_mod) %&amp;gt;% 
  fit_resamples(
    resamples = iris_boots,
    control = control_resamples(save_pred = TRUE)
    )
collect_metrics(rf_results)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   .metric  .estimator  mean     n std_err
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 accuracy multiclass 0.945    25 0.00764
## 2 roc_auc  hand_till  0.993    25 0.00126&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here is a look at the confusion matrix summaries for our model. The confusion matrix let’s us see the correct and incorrect predictions of our models in a single table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glmnet_results %&amp;gt;%
  conf_mat_resampled() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##   Prediction Truth       Freq
##   &amp;lt;fct&amp;gt;      &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 setosa     setosa     13.3 
## 2 setosa     versicolor  0   
## 3 setosa     virginica   0   
## 4 versicolor setosa      0   
## 5 versicolor versicolor 12.2 
## 6 versicolor virginica   1   
## 7 virginica  setosa      0   
## 8 virginica  versicolor  1.24
## 9 virginica  virginica  12.4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_results %&amp;gt;%
  conf_mat_resampled() &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##   Prediction Truth       Freq
##   &amp;lt;fct&amp;gt;      &amp;lt;fct&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 setosa     setosa     13.3 
## 2 setosa     versicolor  0   
## 3 setosa     virginica   0   
## 4 versicolor setosa      0   
## 5 versicolor versicolor 12.3 
## 6 versicolor virginica   1.04
## 7 virginica  setosa      0   
## 8 virginica  versicolor  1.16
## 9 virginica  virginica  12.4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ROC curve helps us visually interpret our model performance at every threshold.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glmnet_results %&amp;gt;%
  collect_predictions() %&amp;gt;%
  group_by(id) %&amp;gt;%
  roc_curve(species, .pred_setosa:.pred_virginica) %&amp;gt;%
  autoplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rf_results %&amp;gt;%
  collect_predictions() %&amp;gt;%
  group_by(id) %&amp;gt;%
  roc_curve(species, .pred_setosa:.pred_virginica) %&amp;gt;%
  autoplot() +
  theme(legend.position = &amp;#39;none&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;
# Final fit
This is it! By using the &lt;code&gt;last_fit(tidy_split)&lt;/code&gt;, we are able to train our model on the training set and test the model on the testing set in one fell swoop! Note, this is the only time we use the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_glmnet &amp;lt;- glmnet_wf %&amp;gt;%
    add_model(glmnet_mod) %&amp;gt;%
    last_fit(tidy_split)

final_glmnet&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Resampling results
## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples  
## # A tibble: 1 x 6
##   splits       id           .metrics      .notes       .predictions    .workflow
##   &amp;lt;list&amp;gt;       &amp;lt;chr&amp;gt;        &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;          &amp;lt;list&amp;gt;   
## 1 &amp;lt;split [113… train/test … &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [37 × … &amp;lt;workflo…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;final_rf &amp;lt;- rf_wf %&amp;gt;%
    add_model(rf_mod) %&amp;gt;%
    last_fit(tidy_split)

final_rf&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # Resampling results
## # Monte Carlo cross-validation (0.75/0.25) with 1 resamples  
## # A tibble: 1 x 6
##   splits       id           .metrics      .notes       .predictions    .workflow
##   &amp;lt;list&amp;gt;       &amp;lt;chr&amp;gt;        &amp;lt;list&amp;gt;        &amp;lt;list&amp;gt;       &amp;lt;list&amp;gt;          &amp;lt;list&amp;gt;   
## 1 &amp;lt;split [113… train/test … &amp;lt;tibble [2 ×… &amp;lt;tibble [0 … &amp;lt;tibble [37 × … &amp;lt;workflo…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;confusion-matrices&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Confusion Matrices&lt;/h1&gt;
&lt;p&gt;Finally, let’s generate a multiclass confusion matrix with the results from the test data. The confusion matrix provides a count of each outcome for all possible outcomes. The columns contain the true values and predictions are set across rows. Notice that the correct predictions are on the diagonal and there were no incorrect predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;collect_metrics(final_glmnet)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy multiclass         1
## 2 roc_auc  hand_till          1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;collect_predictions(final_glmnet) %&amp;gt;%
  conf_mat(species, .pred_class) %&amp;gt;% 
  autoplot(type = &amp;#39;heatmap&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;collect_metrics(final_rf)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy multiclass     0.973
## 2 roc_auc  hand_till      1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;collect_predictions(final_rf) %&amp;gt;%
  conf_mat(species, .pred_class) %&amp;gt;% 
  autoplot(type = &amp;#39;heatmap&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-10-21-iris-data/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Final thoughts&lt;/h1&gt;
&lt;p&gt;It is always good to ask the question, ‘Do my results make sense’? Both models show near perfect predictive power but are they really that good? From our visual analysis, we can confidently say yes, there is a clear distinction between species when comparing the explanatory features. Now that we have a model, we have the power to predict iris type on new data.&lt;/p&gt;
&lt;p&gt;I would like to thank my mentors, the three noble Jedi Knights of the tidyverse and tidymodels: Julia Silge, David Robinson and Andrew Couch.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
